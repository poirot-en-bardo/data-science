---
title: "Abalone"
output: html_notebook
---


```{r}
data <- read.csv("abalone.data")
names <- c("sex", "length", "diameter", "height","whole_weight", "schucked_weight", "viscera_weight","shell_weight", "rings")
colnames(data) <- names
```

```{r}
head(data)
```

```{r}

data <- data[, -which(names(data) == "sex")]

# Load the required library for plotting
library(ggplot2)

# Plot the histogram of Rings variable with default bins
ggplot(data = data, aes(x = rings)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  labs(x = "Rings", y = "Frequency", title = "Distribution of Rings in Abalone Dataset")

```


```{r}
# Create a new column 'age_class' for age bins
data$age_class <- cut(data$rings, breaks = c(0, 10, 15, Inf), labels = c("1", "2", "3"))

# Verify the new column
table(data$age_class)

data <- data[, -which(names(data) == "rings")]
```



```{r}
head(data)
```



## Decision Trees
```{r}
set.seed(2)
data <- na.omit(data)
train <- sample (1:nrow(data), nrow (data)/2)
data.train <- data [train ,]
data.test <- data [-train ,]
```


```{r}
names <- names(data.train)
class_col <- which(names(data) == "age_class")
train_scaled <- scale(data.train[, -class_col])
train <- data.frame(train_scaled, data.train[, class_col])
names(train) <- names
head(train)
test <- data.frame(scale(data.test[, -class_col], center = attr(train_scaled,"scaled:center"), scale = attr(train_scaled, "scaled:scale")), data.test[, class_col])
names(test) <- names
head(test)
```

```{r}
library (tree)
tree1 <- tree(age_class ~ ., data = train )
tree1.pruned <- prune.tree(tree1, best = 3)
plot(tree1)
text(tree1) # gives a graphical representation description of the model

plot(tree1.pruned)
text(tree1.pruned)
```


###Predition
```{r}
tree1.pred <- predict(tree1, test, type ="class")
prune.pred <- predict(tree1.pruned, test, type ="class")
table.tree <- table (tree1.pred, test$age_class)
table.pruned <- table (prune.pred, test$age_class)
accuracy.tree <- sum(diag(table.tree)) / sum(table.tree)
accuracy.pruned <- sum(diag(table.pruned)) / sum(table.pruned)
accuracy.tree
accuracy.pruned
```



## Random Forest
```{r}
library(randomForest) 
m = as.integer(sqrt(ncol(data) - 1))
forest <- randomForest(age_class ~ ., data=train, mtry=m, importance=TRUE)
forest

yhat.bag <- predict(forest, newdata=test)
table <- table (yhat.bag, test$age_class)
accuracy.tree <- sum(diag(table)) / sum(table)
accuracy.tree
```

##SVMs

```{r}
library(e1071)
# Train the SVM model on the 'age_class' variable
svm_model <- svm(age_class ~ ., data = train, type = 'C-classification', kernel = 'radial')

# Make predictions on the test set
predictions <- predict(svm_model, test[,-class_col])  # Exclude the target variable for prediction

# Evaluate the model's performance with a confusion matrix
conf_matrix <- table(predictions, test$age_class)
print(conf_matrix)

# Calculate overall accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Overall Accuracy:", accuracy))
```


##Neural Networks
```{r}
library(nnet)
set.seed(1)
nn.abalone <- nnet (age_class ~., data = train, size = 10, decay = 0.1, maxit = 500) #size of the hidden layer ( 1 layer with 3 units);
#it will know from the data if it's regression or if it's a factor with how many classes
summary (nn.abalone)
nn.train.pred <- predict (nn.abalone, train, type="class") #identify it as class, otherwise it will give a numeric value
# nn.cancer.predict.train <- as.factor (nn.cancer.predict.train)
table <- table(nn.train.pred, test$age_class)
table
accuracy <- sum(diag(table)) / sum(table)
accuracy

nn.abalone.predict.test <- predict(nn.abalone, data = test, type="class") 
table.test <- table(nn.abalone.predict.test, test$age_class)
table.test
accuracy.test <- sum(diag(table.test)) / sum(table.test)
accuracy.test
```


```{r}

# Train the neural network with multiple hidden layers
nn.abalone <- nnet(age_class ~ ., data = train, size = c(4, 6, 4, 2)) 
summary(nn.abalone)

# Make predictions on the training set
nn.train.pred <- predict(nn.abalone, train, type = "class")
table_train <- table(nn.train.pred, train$age_class)
accuracy_train <- sum(diag(table_train)) / sum(table_train)
print(paste("Training Accuracy:", accuracy_train))

# Make predictions on the test set
nn.abalone.predict.test <- predict(nn.abalone, data = test, type = "class") 
table_test <- table(nn.abalone.predict.test, test$age_class)
accuracy_test <- sum(diag(table_test)) / sum(table_test)
print(paste("Test Accuracy:", accuracy_test))

```

```{r}
library(keras)

num_features <- ncol(data.train) - 1
num_classes <- length(unique(data.train$age_class))

# Build the model
model <- keras_model_sequential() %>%
  layer_dense(units = 10, input_shape = c(7), name = "Input_Layer") %>%
  layer_activation('relu') %>% 
  layer_dense(units = 6, name = "Hidden_Layer_1") %>%
  layer_activation('relu') %>% 
  layer_dense(units = num_classes, name = "Output_Layer")%>%
  layer_activation('softmax')


model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
)

# Prepare labels as categorical
train_labels <- to_categorical(data.train$age_class)
test_labels <- to_categorical(data.test$age_class)

# Fit the model
history <- model %>% fit(
  as.matrix(train[-1]), train_labels,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2
)

# Evaluate the model
score_train <- model %>% evaluate(as.matrix(train[-1]), train_labels)
score_test <- model %>% evaluate(as.matrix(test[-1]), test_labels)
cat('Training Accuracy:', score_train$accuracy, "\n")
cat('Test Accuracy:', score_test$accuracy, "\n")

```


