data <- read.csv("abalone.data")
names <- c("sex", "length", "diameter", "height","whole_weight", "schucked_weight", "viscera_weight","shell_weight", "rings")
colnames(data) <- names
head(data)
data <- data[, -which(names(data) == "sex")]
# Load the required library for plotting
library(ggplot2)
# Plot the histogram of Rings variable with default bins
ggplot(data = data, aes(x = rings)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
labs(x = "Rings", y = "Frequency", title = "Distribution of Rings in Abalone Dataset")
# Create a new column 'age_class' for age bins
data$age_class <- cut(data$rings, breaks = c(0, 5, 10, 15, 20, Inf), labels = c("1", "2", "3", "4", "5"))
# Verify the new column
table(data$age_class)
data <- data[, -which(names(data) == "rings")]
head(data)
set.seed(2)
data <- na.omit(data)
train <- sample (1:nrow(data), nrow (data)/2)
data.train <- data [train ,]
data.test <- data [-train ,]
names <- names(data.train)
class_col <- which(names(data) == "age_class")
train_scaled <- scale(data.train[, -class_col])
train <- data.frame(train_scaled, data.train[, class_col])
names(train) <- names
head(train)
test <- data.frame(scale(data.test[, -class_col], center = attr(train_scaled,"scaled:center"), scale = attr(train_scaled, "scaled:scale")), data.test[, class_col])
names(test) <- names
head(test)
library (tree)
tree1 <- tree(sex ~ ., data = train )
library (tree)
tree1 <- tree(age_class ~ ., data = train )
tree1.pruned <- prune.tree(tree1, best = 3)
plot(tree1)
text(tree1) # gives a graphical representation description of the model
plot(tree1.pruned)
text(tree1.pruned)
library (tree)
tree1 <- tree(age_class ~ ., data = train )
tree1.pruned <- prune.tree(tree1, best = 3)
plot(tree1)
text(tree1) # gives a graphical representation description of the model
plot(tree1.pruned)
text(tree1.pruned)
tree1.pred <- predict(tree1, test, type ="class")
prune.pred <- predict(tree1.pruned, test, type ="class")
table.tree <- table (tree1.pred, test$age_class)
table.pruned <- table (prune.pred, test$age_class)
accuracy.tree <- sum(diag(table.tree)) / sum(table.tree)
accuracy.pruned <- sum(diag(table.pruned)) / sum(table.pruned)
accuracy.tree
accuracy.pruned
library(randomForest)
m = as.integer(sqrt(ncol(data) - 1))
forest <- randomForest(sex ~ ., data=train, mtry=m, importance=TRUE)
library(randomForest)
m = as.integer(sqrt(ncol(data) - 1))
forest <- randomForest(age_class ~ ., data=train, mtry=m, importance=TRUE)
forest
yhat.bag <- predict(forest, newdata=test)
table <- table (yhat.bag, test$sex)
library(randomForest)
m = as.integer(sqrt(ncol(data) - 1))
forest <- randomForest(age_class ~ ., data=train, mtry=m, importance=TRUE)
forest
yhat.bag <- predict(forest, newdata=test)
table <- table (yhat.bag, test$age_class)
accuracy.tree <- sum(diag(table)) / sum(table)
accuracy.tree
library(e1071)
# Train the SVM model on the 'Sex' variable
svm_model <- svm(age_class ~ ., data = train, type = 'C-classification', kernel = 'radial')
# Make predictions on the test set
predictions <- predict(svm_model, test[,-1])  # Exclude the target variable for prediction
library(e1071)
# Train the SVM model on the 'Sex' variable
svm_model <- svm(age_class ~ ., data = train, type = 'C-classification', kernel = 'radial')
# Make predictions on the test set
predictions <- predict(svm_model, test[,-1])  # Exclude the target variable for prediction
library(e1071)
# Train the SVM model on the 'Sex' variable
svm_model <- svm(age_class ~ ., data = train, type = 'C-classification', kernel = 'radial')
# Make predictions on the test set
predictions <- predict(svm_model, test[,-1])  # Exclude the target variable for prediction
library(e1071)
# Train the SVM model on the 'Sex' variable
svm_model <- svm(age_class ~ ., data = train, type = 'C-classification', kernel = 'radial')
# Make predictions on the test set
predictions <- predict(svm_model, test[,-1, drop = FALSE])  # Exclude the target variable for prediction
library(nnet)
set.seed(1)
nn.abalone <- nnet (sex ~., data = train, size = 10) #size of the hidden layer ( 1 layer with 3 units);
library(nnet)
set.seed(1)
nn.abalone <- nnet (age_class ~., data = train, size = 10) #size of the hidden layer ( 1 layer with 3 units);
#it will know from the data if it's regression or if it's a factor with how many classes
summary (nn.abalone)
nn.train.pred <- predict (nn.abalone, train, type="class") #identify it as class, otherwise it will give a numeric value
# nn.cancer.predict.train <- as.factor (nn.cancer.predict.train)
table <- table(nn.train.pred, test$age_class)
table
accuracy <- sum(diag(table)) / sum(table)
accuracy
nn.abalone.predict.test <- predict(nn.abalone, data = test, type="class")
table.test <- table(nn.abalone.predict.test, test$age_class)
table.test
accuracy.test <- sum(diag(table.test)) / sum(table.test)
accuracy.test
library(nnet)
set.seed(1)
nn.abalone <- nnet (age_class ~., data = train, size = 10, decay = 0.1, maxit = 500) #size of the hidden layer ( 1 layer with 3 units);
#it will know from the data if it's regression or if it's a factor with how many classes
summary (nn.abalone)
nn.train.pred <- predict (nn.abalone, train, type="class") #identify it as class, otherwise it will give a numeric value
# nn.cancer.predict.train <- as.factor (nn.cancer.predict.train)
table <- table(nn.train.pred, test$age_class)
table
accuracy <- sum(diag(table)) / sum(table)
accuracy
nn.abalone.predict.test <- predict(nn.abalone, data = test, type="class")
table.test <- table(nn.abalone.predict.test, test$age_class)
table.test
accuracy.test <- sum(diag(table.test)) / sum(table.test)
accuracy.test
library(nnet)
set.seed(1)
nn.abalone <- nnet (age_class ~., data = train, size = 20, decay = 0.1, maxit = 500) #size of the hidden layer ( 1 layer with 3 units);
#it will know from the data if it's regression or if it's a factor with how many classes
summary (nn.abalone)
nn.train.pred <- predict (nn.abalone, train, type="class") #identify it as class, otherwise it will give a numeric value
# nn.cancer.predict.train <- as.factor (nn.cancer.predict.train)
table <- table(nn.train.pred, test$age_class)
table
accuracy <- sum(diag(table)) / sum(table)
accuracy
nn.abalone.predict.test <- predict(nn.abalone, data = test, type="class")
table.test <- table(nn.abalone.predict.test, test$age_class)
table.test
accuracy.test <- sum(diag(table.test)) / sum(table.test)
accuracy.test
library(nnet)
set.seed(1)
nn.abalone <- nnet (age_class ~., data = train, size = 10, decay = 0.1, maxit = 500) #size of the hidden layer ( 1 layer with 3 units);
#it will know from the data if it's regression or if it's a factor with how many classes
summary (nn.abalone)
nn.train.pred <- predict (nn.abalone, train, type="class") #identify it as class, otherwise it will give a numeric value
# nn.cancer.predict.train <- as.factor (nn.cancer.predict.train)
table <- table(nn.train.pred, test$age_class)
table
accuracy <- sum(diag(table)) / sum(table)
accuracy
nn.abalone.predict.test <- predict(nn.abalone, data = test, type="class")
table.test <- table(nn.abalone.predict.test, test$age_class)
table.test
accuracy.test <- sum(diag(table.test)) / sum(table.test)
accuracy.test
# Train the neural network with multiple hidden layers
nn.abalone <- nnet(sex ~ ., data = train, size = c(4, 6, 4, 2))
# Train the neural network with multiple hidden layers
nn.abalone <- nnet(age_class ~ ., data = train, size = c(4, 6, 4, 2))
summary(nn.abalone)
# Make predictions on the training set
nn.train.pred <- predict(nn.abalone, train, type = "class")
library(keras)
install.packages("keras")
library(keras)
# Building a more complex model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'relu', input_shape = ncol(train[-1])) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_batch_normalization() %>%
layer_dense(units = length(unique(train$age_class)), activation = 'softmax')
install_tensorflow()
library(keras)
install_tensorflow()
library(reticulate)
py_install("tensorflow", envname = "r-reticulate")
library(keras)
# Building a more complex model
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = 'relu', input_shape = ncol(train[-1])) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_batch_normalization() %>%
layer_dense(units = length(unique(train$age_class)), activation = 'softmax')
install_tensorflow()
library(reticulate)
py_install("tensorflow", envname = "r-reticulate")
library(keras)
tensorflow::tf_config()
# Train the neural network with multiple hidden layers
nn.abalone <- nnet(age_class ~ ., data = train, size = c(4, 6, 4, 2))
summary(nn.abalone)
# Make predictions on the training set
nn.train.pred <- predict(nn.abalone, train, type = "class")
library(keras)
# Build the model
model <- keras_model_sequential() %>%
layer_dense(units = 4, activation = 'relu', input_shape = ncol(train[-1])) %>%
layer_dense(units = 6, activation = 'relu') %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 2, activation = 'relu') %>%
layer_dense(units = length(unique(train$age_class)), activation = 'softmax')
install_tensorflow()
# install.packages("remotes")
remotes::install_github("rstudio/tensorflow")
install.packages("remotes")
# install.packages("remotes")
remotes::install_github("rstudio/tensorflow")
library(keras)
# Build the model
model <- keras_model_sequential() %>%
layer_dense(units = 4, activation = 'relu', input_shape = ncol(train[-1])) %>%
layer_dense(units = 6, activation = 'relu') %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 2, activation = 'relu') %>%
layer_dense(units = length(unique(train$age_class)), activation = 'softmax')
library(tensorflow)
library(tensorflow)
